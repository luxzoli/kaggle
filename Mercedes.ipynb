{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#import xgboost as xgb\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.linear_model import ElasticNetCV, LassoLarsCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class StackingEstimator(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.estimator.fit(X, y, **fit_params)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = check_array(X)\n",
    "        X_transformed = np.copy(X)\n",
    "        # add class probabilities as a synthetic feature\n",
    "        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n",
    "            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n",
    "\n",
    "        # add class prodiction as a synthetic feature\n",
    "        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "for c in train.columns:\n",
    "    if train[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train[c].values) + list(test[c].values))\n",
    "        train[c] = lbl.transform(list(train[c].values))\n",
    "        test[c] = lbl.transform(list(test[c].values))\n",
    "\n",
    "\n",
    "\n",
    "n_comp = 12\n",
    "n_neighbors = 30\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=420)\n",
    "tsvd_results_train = tsvd.fit_transform(train.drop([\"y\"], axis=1))\n",
    "tsvd_results_test = tsvd.transform(test)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=420)\n",
    "pca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\n",
    "pca2_results_test = pca.transform(test)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=420)\n",
    "ica2_results_train = ica.fit_transform(train.drop([\"y\"], axis=1))\n",
    "ica2_results_test = ica.transform(test)\n",
    "\n",
    "# GRP\n",
    "grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\n",
    "grp_results_train = grp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "grp_results_test = grp.transform(test)\n",
    "\n",
    "# SRP\n",
    "srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n",
    "srp_results_train = srp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "srp_results_test = srp.transform(test)\n",
    "\n",
    "clf = manifold.Isomap(n_neighbors, n_components=3)\n",
    "X_iso = clf.fit_transform(train.drop([\"y\"], axis=1))\n",
    "X_iso_t = clf.transform(test)\n",
    "\n",
    "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=3,\n",
    "                                      method='standard')\n",
    "X_lle = clf.fit_transform(train.drop([\"y\"], axis=1))\n",
    "X_lle_t = clf.transform(test)\n",
    "\n",
    "#clf = manifold.MDS(n_components=n_comp, n_init=1, max_iter=100)\n",
    "#X_mds = clf.fit_transform(train.drop([\"y\"], axis=1))\n",
    "#X_mds_t = clf.transform(test)\n",
    "\n",
    "#save columns list before adding the decomposition components\n",
    "\n",
    "usable_columns = list(set(train.columns) - set(['y']))\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "for i in range(1, n_comp + 1):\n",
    "    train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n",
    "    test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n",
    "\n",
    "    train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n",
    "    test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n",
    "\n",
    "    train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "    test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "\n",
    "    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
    "    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
    "\n",
    "    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
    "    test['srp_' + str(i)] = srp_results_test[:, i - 1]\n",
    "    if n_comp < 4:\n",
    "        train['iso_' + str(i)] = X_iso[:, i - 1]\n",
    "        test['iso_' + str(i)] = X_iso_t[:, i - 1]\n",
    "        \n",
    "        train['lle_' + str(i)] = X_lle[:, i - 1]\n",
    "        test['lle_' + str(i)] = X_lle_t[:, i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.33140762  0.21845297  0.07507684]\n",
      "[ 0.57778623  0.49634811  0.57653935]\n",
      "[ 0.47880328  0.40876998  0.49613045]\n",
      "[ 0.57069378  0.48050593  0.55661164]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "train = train.sample(frac=1.0)\n",
    "y_train = train['y'].values\n",
    "\n",
    "clf = MLPRegressor(hidden_layer_sizes = (50,),activation=\"relu\")\n",
    "print(cross_val_score(clf,train.drop('y', axis=1),y_train,scoring=\"r2\",n_jobs=3))\n",
    "\n",
    "params = {'n_estimators': 600, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf = GradientBoostingRegressor(**params)\n",
    "print(cross_val_score(clf,train.drop('y', axis=1),y_train,scoring=\"r2\",n_jobs=3))\n",
    "\n",
    "clf = RandomForestRegressor()\n",
    "print(cross_val_score(clf,train.drop('y', axis=1),y_train,scoring=\"r2\",n_jobs=3))\n",
    "\n",
    "clf = LassoLarsCV()\n",
    "print(cross_val_score(clf,train.drop('y', axis=1),y_train,scoring=\"r2\",n_jobs=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "id_test = test['ID'].values\n",
    "#finaltrainset and finaltestset are data to be used only the stacked model (does not contain PCA, SVD... arrays) \n",
    "finaltrainset = train[usable_columns].values\n",
    "finaltestset = test[usable_columns].values\n",
    "clf = GradientBoostingRegressor(**params)\n",
    "clf.fit(finaltrainset,y_train)\n",
    "y_pred = clf.predict(finaltestset)\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = y_pred\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funktioniert gerade nicht so gut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.137e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.082e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:381: RuntimeWarning: overflow encountered in true_divide\n",
      "  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny))\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:377: RuntimeWarning: overflow encountered in true_divide\n",
      "  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny))\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=3.578e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=2.949e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=2.949e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.812e-03, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.812e-03, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=2.542e-03, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.534e-03, with an active set of 44 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=2.506e-03, with an active set of 46 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=2.342e-03, with an active set of 52 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=2.339e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.007e-03, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 67 iterations, alpha=1.976e-03, previous alpha=1.963e-03, with an active set of 66 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.318e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.394e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.758e-03, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=2.188e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=2.050e-03, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.768e-03, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=1.697e-03, with an active set of 69 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=1.696e-03, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.675e-03, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=1.552e-03, with an active set of 75 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.532e-03, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 81 iterations, alpha=1.551e-03, previous alpha=1.504e-03, with an active set of 76 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.134e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.041e-03, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.727e-03, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.547e-03, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.255e-03, with an active set of 50 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=2.076e-03, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=2.076e-03, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=2.076e-03, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Zoli\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 60 iterations, alpha=2.057e-03, previous alpha=2.041e-03, with an active set of 59 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "stacked_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=GradientBoostingRegressor(**params)),\n",
    "    LassoLarsCV()\n",
    "\n",
    ")\n",
    "\n",
    "stacked_pipeline.fit(finaltrainset, y_train)\n",
    "y_pred = stacked_pipeline.predict(finaltestset)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noch nicht ausprobiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#usable_columns = list(set(train.columns) - set(['y']))\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "y_train = train['y'].values\n",
    "y_mean = np.mean(y_train)\n",
    "id_test = test['ID'].values\n",
    "#finaltrainset and finaltestset are data to be used only the stacked model (does not contain PCA, SVD... arrays) \n",
    "finaltrainset = train[usable_columns].values\n",
    "finaltestset = test[usable_columns].values\n",
    "\n",
    "\n",
    "'''Train the xgb model then predict the test data'''\n",
    "\n",
    "xgb_params = {\n",
    "    'n_trees': 520, \n",
    "    'eta': 0.0045,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.93,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1\n",
    "}\n",
    "# NOTE: Make sure that the class is labeled 'class' in the data file\n",
    "\n",
    "dtrain = xgb.DMatrix(train.drop('y', axis=1), y_train)\n",
    "dtest = xgb.DMatrix(test)\n",
    "\n",
    "num_boost_rounds = 1250\n",
    "# train model\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "\n",
    "clf.fit(train.drop('y', axis=1), y_train)\n",
    "mse = mean_squared_error(y_test, clf.predict(X_test))\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "\n",
    "y_pred = model.predict(dtest)\n",
    "\n",
    "'''Train the stacked models then predict the test data'''\n",
    "\n",
    "stacked_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
    "    StackingEstimator(estimator=GradientBoostingRegressor(learning_rate=0.001, loss=\"huber\", max_depth=3, max_features=0.55, min_samples_leaf=18, min_samples_split=14, subsample=0.7)),\n",
    "    LassoLarsCV()\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "stacked_pipeline.fit(finaltrainset, y_train)\n",
    "results = stacked_pipeline.predict(finaltestset)\n",
    "\n",
    "'''R2 Score on the entire Train data when averaging'''\n",
    "\n",
    "print('R2 score on train data:')\n",
    "print(r2_score(y_train,stacked_pipeline.predict(finaltrainset)*0.2855 + model.predict(dtrain)*0.7145))\n",
    "\n",
    "'''Average the preditionon test data  of both models then save it on a csv file'''\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = y_pred*0.75 + results*0.25\n",
    "sub.to_csv('stacked-models.csv', index=False)\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
